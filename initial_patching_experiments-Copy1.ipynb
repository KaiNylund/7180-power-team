{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad8560f-962a-4d19-a401-0f72870eff12",
   "metadata": {},
   "source": [
    "\n",
    "Activation Patching Experiment: Power Dynamics in LLM Representations\n",
    "=====================================================================\n",
    "\n",
    "In this notebook I am trying to recreate the figures from the week 5 papers on patching, that show the causal influence of each layer on the outcome of the prompt. Claude was heavily involved in taking [the code from the ndif/nnsight website](https://nnsight.net/notebooks/mini-papers/marks_geometry_of_truth/#%E2%9E%A1%EF%B8%8F-Let's-scale-things-up!-Steering-Llama-70B-on-NDIF) and making it operational in this context.\n",
    "\n",
    "The code reproduces the methodology of Figure 2 from [Marks & Tegmark (2023) \"The Geometry of Truth\"](https://arxiv.org/abs/2310.06824) paper — adapted for power-dynamic sentence pairs.\n",
    "\n",
    "For each (token_position, layer), we patch residual stream activations from a \"source\" prompt (entity HAS power) into a \"base\" prompt (entity LACKS power), and measure log P(TRUE) - log P(FALSE) to build a causal heatmap of where the model stores power-relationship information.\n",
    "\n",
    "\n",
    "Requirements:\n",
    "    pip install nnsight torch matplotlib pandas tqdm numpy\n",
    "    + for remote: NDIF API key from https://login.ndif.us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1a0b0-6e50-4979-82d2-33929f857410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from nnsight import LanguageModel\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ced825-809c-4929-a970-3a07e5fde0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ────────────────────────────────────────────────────────\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "# Set to True for NDIF remote execution, False for local GPU\n",
    "USE_REMOTE = True\n",
    "\n",
    "# For remote: set your NDIF API key (only needed once)\n",
    "from nnsight import CONFIG\n",
    "CONFIG.set_default_api_key(\"YOUR_API_KEY\")\n",
    "\n",
    "# For local: set device and dtype\n",
    "# DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7cd18-d718-4168-b5d0-cc26bae6749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load model ───────────────────────────────────────────────────────────\n",
    "\n",
    "if USE_REMOTE:\n",
    "    model = LanguageModel(MODEL_ID, device_map=\"auto\")\n",
    "else:\n",
    "    model = LanguageModel(MODEL_ID, device_map=DEVICE, torch_dtype=torch.float16)\n",
    "\n",
    "clear_output()\n",
    "print(f\"Loaded {MODEL_ID} ({'remote' if USE_REMOTE else 'local'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facab479-9bb6-40f5-971a-d20cddf1c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load sentence pairs from TSV ─────────────────────────────────────────\n",
    "\n",
    "TSV_PATH = \"FILE_PATH_TO_CONTRASTIVE_PAIRED_SENTENCES.tsv\"\n",
    "\n",
    "pairs_df = pd.read_csv(TSV_PATH, sep=\"\\t\")\n",
    "assert all(c in pairs_df.columns for c in [\"name\", \"power\", \"no_power\"]), \\\n",
    "    \"TSV must have columns: name, power, no_power\"\n",
    "\n",
    "print(f\"Loaded {len(pairs_df)} sentence pairs from {TSV_PATH}\")\n",
    "print(pairs_df.head())\n",
    "\n",
    "QUESTION_TEMPLATE = (\n",
    "    \"\"\"In the following sentence, {entity} exercises power over someone or something else. TRUE or FALSE? Sentence: {sentence} Answer:\"\"\"\n",
    ")\n",
    "\n",
    "PAIR_INDEX = 0\n",
    "row = pairs_df.iloc[PAIR_INDEX]\n",
    "ENTITY = row[\"name\"]\n",
    "SOURCE_SENTENCE = row[\"power\"]\n",
    "BASE_SENTENCE = row[\"no_power\"]\n",
    "\n",
    "source_prompt = QUESTION_TEMPLATE.format(entity=ENTITY, sentence=SOURCE_SENTENCE)\n",
    "base_prompt = QUESTION_TEMPLATE.format(entity=ENTITY, sentence=BASE_SENTENCE)\n",
    "\n",
    "print(f\"\\n── Pair {PAIR_INDEX}: entity = '{ENTITY}' ──\")\n",
    "print(f\"SOURCE (has power):  {source_prompt}\")\n",
    "print(f\"BASE (lacks power):  {base_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ba57d-13c1-4f6f-9ff2-17ddc1ca784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Tokenize & align prompts ────────────────────────────────────────────\n",
    "\n",
    "source_ids = model.tokenizer(source_prompt, return_tensors=\"pt\").input_ids[0]\n",
    "base_ids = model.tokenizer(base_prompt, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "len_source = len(source_ids)\n",
    "len_base = len(base_ids)\n",
    "max_len = max(len_source, len_base)\n",
    "\n",
    "pad_id = model.tokenizer.pad_token_id\n",
    "if pad_id is None:\n",
    "    pad_id = model.tokenizer.eos_token_id\n",
    "\n",
    "def right_align(ids, target_len, pad_id):\n",
    "    pad_count = target_len - len(ids)\n",
    "    if pad_count > 0:\n",
    "        return torch.cat([torch.full((pad_count,), pad_id, dtype=ids.dtype), ids])\n",
    "    return ids\n",
    "\n",
    "source_ids_aligned = right_align(source_ids, max_len, pad_id)\n",
    "base_ids_aligned = right_align(base_ids, max_len, pad_id)\n",
    "\n",
    "# Find where the sentence starts by looking for \"Sentence:\" in the tokens\n",
    "base_token_list = [model.tokenizer.decode([base_ids_aligned[i]]) for i in range(max_len)]\n",
    "\n",
    "sentence_start = None\n",
    "for i in range(max_len - 1):\n",
    "    if \"Sentence\" in base_token_list[i] and \":\" in base_token_list[i + 1]:\n",
    "        sentence_start = i + 2\n",
    "        break\n",
    "    elif \"Sentence:\" in base_token_list[i]:\n",
    "        sentence_start = i + 1\n",
    "        break\n",
    "    elif base_token_list[i].strip().endswith(\"Sentence:\"):\n",
    "        sentence_start = i + 1\n",
    "        break\n",
    "\n",
    "if sentence_start is None:\n",
    "    for i in range(max_len):\n",
    "        if source_ids_aligned[i] != base_ids_aligned[i]:\n",
    "            sentence_start = i\n",
    "            break\n",
    "\n",
    "base_token_strings = []\n",
    "for i in range(sentence_start, max_len):\n",
    "    tok = model.tokenizer.decode([base_ids_aligned[i]])\n",
    "    base_token_strings.append(tok)\n",
    "\n",
    "print(f\"\\nSource tokens: {len_source}, Base tokens: {len_base}, Aligned length: {max_len}\")\n",
    "print(f\"Sentence starts at position: {sentence_start}\")\n",
    "print(f\"Patching over positions {sentence_start} to {max_len - 1} ({len(base_token_strings)} tokens)\")\n",
    "print(f\"\\nBase tokens being patched over:\")\n",
    "for i, t in enumerate(base_token_strings):\n",
    "    print(f\"  pos {sentence_start + i}: '{t}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606c313-3fe8-4707-bdc4-20b765d87744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Verify model outputs ────────────────────────────────────────────────\n",
    "\n",
    "yes_variants = [\" TRUE\", \" True\", \" true\", \"TRUE\", \"True\", \"true\"]\n",
    "no_variants = [\" FALSE\", \" False\", \" false\", \"FALSE\", \"False\", \"false\"]\n",
    "\n",
    "yes_token_ids = []\n",
    "no_token_ids = []\n",
    "for v in yes_variants:\n",
    "    ids = model.tokenizer(v, add_special_tokens=False).input_ids\n",
    "    if len(ids) == 1:\n",
    "        yes_token_ids.append(ids[0])\n",
    "for v in no_variants:\n",
    "    ids = model.tokenizer(v, add_special_tokens=False).input_ids\n",
    "    if len(ids) == 1:\n",
    "        no_token_ids.append(ids[0])\n",
    "\n",
    "yes_token_ids = list(set(yes_token_ids))\n",
    "no_token_ids = list(set(no_token_ids))\n",
    "\n",
    "print(f\"\\nTRUE token ids: {yes_token_ids}\")\n",
    "print(f\"  variants: {[model.tokenizer.decode([t]) for t in yes_token_ids]}\")\n",
    "print(f\"FALSE token ids: {no_token_ids}\")\n",
    "print(f\"  variants: {[model.tokenizer.decode([t]) for t in no_token_ids]}\")\n",
    "\n",
    "for name, ids in [(\"SOURCE\", source_ids_aligned), (\"BASE\", base_ids_aligned)]:\n",
    "    if USE_REMOTE:\n",
    "        with model.trace(ids.unsqueeze(0), remote=True):\n",
    "            logits = model.output.logits.save()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            with model.trace(ids.unsqueeze(0)):\n",
    "                logits = model.output.logits.save()\n",
    "    probs = logits[0, -1].softmax(dim=-1)\n",
    "    p_yes = sum(probs[t].item() for t in yes_token_ids)\n",
    "    p_no = sum(probs[t].item() for t in no_token_ids)\n",
    "    top_tok = model.tokenizer.decode(logits.argmax(dim=-1)[0, -1])\n",
    "    log_diff = torch.log(torch.tensor(p_yes) / torch.tensor(p_no)).item()\n",
    "    print(f\"{name}: P(TRUE)={p_yes:.4f}, P(FALSE)={p_no:.4f}, \"\n",
    "          f\"log P(TRUE)-log P(FALSE)={log_diff:.3f}, \"\n",
    "          f\"top token='{top_tok}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95771060-9577-4ff6-bed2-634e1ecd8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cache source activations ────────────────────────────────────────────\n",
    "\n",
    "num_layers = model.config.num_hidden_layers\n",
    "print(f\"\\nModel has {num_layers} layers. Caching source activations...\")\n",
    "\n",
    "source_activations = []\n",
    "\n",
    "if USE_REMOTE:\n",
    "    for i in range(num_layers):\n",
    "        with model.trace(source_ids_aligned.unsqueeze(0), remote=True):\n",
    "            act = model.model.layers[i].output[0].save()\n",
    "        if act.dim() == 2:\n",
    "            act = act.unsqueeze(0)\n",
    "        source_activations.append(act)\n",
    "        if (i + 1) % 8 == 0:\n",
    "            print(f\"  Cached {i + 1}/{num_layers} layers...\")\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        with model.trace(source_ids_aligned.unsqueeze(0)):\n",
    "            for layer in model.model.layers:\n",
    "                source_activations.append(layer.output[0].clone().save())\n",
    "    for i in range(len(source_activations)):\n",
    "        if source_activations[i].dim() == 2:\n",
    "            source_activations[i] = source_activations[i].unsqueeze(0)\n",
    "\n",
    "print(f\"Source activations cached: {len(source_activations)} layers\")\n",
    "print(f\"Shape: {source_activations[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548d279-24ee-4411-b8b1-e9a9e04be405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Run patching experiment ──────────────────────────────────────────────\n",
    "\n",
    "num_tokens = len(base_token_strings)\n",
    "print(f\"\\nRunning patching: {num_layers} layers × {num_tokens} token positions...\")\n",
    "\n",
    "patching_results = []\n",
    "\n",
    "for layer_idx in trange(num_layers, desc=\"Layers\"):\n",
    "    layer_results = []\n",
    "    for token_idx in range(sentence_start, sentence_start + num_tokens):\n",
    "        if USE_REMOTE:\n",
    "            with model.trace(base_ids_aligned.unsqueeze(0), remote=True):\n",
    "                layer_out = model.model.layers[layer_idx].output[0]\n",
    "                src_act = source_activations[layer_idx]\n",
    "                if layer_out.dim() == 2:\n",
    "                    layer_out[token_idx, :] = src_act[0, token_idx, :]\n",
    "                else:\n",
    "                    layer_out[:, token_idx, :] = src_act[:, token_idx, :]\n",
    "                patched_logits = model.output.logits.save()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                with model.trace(base_ids_aligned.unsqueeze(0)):\n",
    "                    layer_out = model.model.layers[layer_idx].output[0]\n",
    "                    src_act = source_activations[layer_idx]\n",
    "                    if layer_out.dim() == 2:\n",
    "                        layer_out[token_idx, :] = src_act[0, token_idx, :]\n",
    "                    else:\n",
    "                        layer_out[:, token_idx, :] = src_act[:, token_idx, :]\n",
    "                    patched_logits = model.output.logits.save()\n",
    "\n",
    "        patched_probs = patched_logits[0, -1].softmax(dim=-1)\n",
    "        p_yes = sum(patched_probs[t].item() for t in yes_token_ids)\n",
    "        p_no = sum(patched_probs[t].item() for t in no_token_ids)\n",
    "        diff = torch.log(torch.tensor(p_yes) / torch.tensor(p_no)).item()\n",
    "        layer_results.append(diff)\n",
    "\n",
    "    patching_results.append(layer_results)\n",
    "\n",
    "print(\"Patching complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5e0c4-1fc4-4e69-90f2-a34690a00018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Plot the heatmap (Figure 2 style) ───────────────────────────────────\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(10, 0.5 * len(base_token_strings)), 8))\n",
    "\n",
    "arr = np.array(patching_results)\n",
    "\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"white_blue\", [\"#FFFFFF\", \"#156082\"])\n",
    "\n",
    "im = ax.imshow(arr, cmap=cmap, aspect=\"auto\", vmin=arr.min(), vmax=arr.max())\n",
    "\n",
    "ax.set_xticks(range(len(base_token_strings)))\n",
    "ax.set_xticklabels([t.strip() for t in base_token_strings], rotation=90,\n",
    "                    ha=\"center\", va=\"top\", fontsize=10)\n",
    "ax.set_yticks(range(num_layers))\n",
    "ax.set_yticklabels(range(num_layers), fontsize=8)\n",
    "ax.set_xlabel(\"Token Position\", fontsize=12)\n",
    "ax.set_ylabel(\"Layer\", fontsize=12)\n",
    "ax.set_title(\n",
    "    f\"Activation Patching: '{BASE_SENTENCE}' ← '{SOURCE_SENTENCE}'\\n\"\n",
    "    f\"log P(TRUE) − log P(FALSE) after patching each (token, layer)\",\n",
    "    fontsize=11\n",
    ")\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label(\"log P(TRUE) − log P(FALSE)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"patching_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved to patching_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799619e6-7f8c-4d6e-bc48-52671e2ed78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 8. Save results ────────────────────────────────────────────────────────\n",
    "\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"entity\": ENTITY,\n",
    "    \"source_prompt\": source_prompt,\n",
    "    \"base_prompt\": base_prompt,\n",
    "    \"source_sentence\": SOURCE_SENTENCE,\n",
    "    \"base_sentence\": BASE_SENTENCE,\n",
    "    \"sentence_start\": sentence_start,\n",
    "    \"token_labels\": base_token_strings,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"patching_results\": patching_results,\n",
    "}\n",
    "\n",
    "with open(\"patching_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to patching_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnsight_env",
   "language": "python",
   "name": "nnsight_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
